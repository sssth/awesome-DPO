# awesome-DPO
Papers related to Direct Preference Optimization（DPO）

## explicit reward model(dataset)  
ADPO - Reinforcement Learning from Human Feedback with Active Queries [[pdf]](https://arxiv.org/pdf/2402.09401)  
APO - Active Preference Learning for Large Language Models [[pdf]](https://arxiv.org/pdf/2402.08114)  
f-DPO - Filtered Direct Preference Optimization [[pdf]](https://arxiv.org/pdf/2404.13846)  
RSO - Statistical Rejection Sampling Improves Preference Optimization [[pdf]](https://arxiv.org/pdf/2309.06657)  
RS-DPO - RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models [[pdf]](https://arxiv.org/pdf/2402.10038)
## explicit reward model(loss)
ODPO - Direct Preference Optimization with an Offset [[pdf]](https://arxiv.org/pdf/2402.10571)  
Sr-DPO - Direct Alignment of Language Models via Quality-Aware Self-Refinement [[pdf]](https://arxiv.org/pdf/2405.21040)  
BRAIN - BRAIN: Bayesian Reward-conditioned Amortized INference for natural language generation from feedback [[pdf]](https://arxiv.org/pdf/2402.02479)  
## pairwise data structure expansion
KTO - KTO: Model Alignment as Prospect Theoretic Optimization [[pdf]](https://arxiv.org/pdf/2402.01306)  
ULMA - ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference [[pdf]](https://openreview.net/pdf?id=E5CMyG6jl0)  
LiPO - LiPO: Listwise Preference Optimization through Learning-to-Rank [[pdf]](https://arxiv.org/pdf/2402.01878)  
Curry-DPO - Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences [[pdf]](https://arxiv.org/pdf/2403.07230)  
BCO - Binary Classifier Optimization for Large Language Model Alignment [[pdf]](https://arxiv.org/pdf/2404.04656)  
## other methods for data
RPO - Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts [[pdf]](https://arxiv.org/pdf/2402.10958)  
D2O - Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization [[pdf]](https://arxiv.org/pdf/2403.03419)  
r-DPO - Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs [[pdf]](https://arxiv.org/pdf/2402.08005)  
XPO - Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF [[pdf]](https://arxiv.org/pdf/2405.21046)  
## online/interactive DPO
D2PO - D2PO: Discriminator-Guided DPO with Response Evaluation Models [[pdf]](https://arxiv.org/pdf/2405.01511)  
OAIF - Direct Language Model Alignment from Online AI Feedback [[pdf]](https://arxiv.org/pdf/2402.04792)  
sDPO - sDPO: Don’t Use Your Data All at Once [[pdf]](https://arxiv.org/pdf/2403.19270)  
TR-DPO - Learn Your Reference Model for Real Good Alignment [[pdf]](https://arxiv.org/pdf/2404.09656)  
Self-Rewarding - Self-Rewarding Language Models [[pdf]](https://arxiv.org/pdf/2401.10020)  
Interactive-DPO - Iterative Reasoning Preference Optimization [[pdf]](https://arxiv.org/pdf/2404.19733)  
## conparison methods
PRO - Preference Ranking Optimization for Human Alignment [[pdf]](https://ojs.aaai.org/index.php/AAAI/article/view/29865/31509)  
SLiC-HF - SLiC-HF: Sequence Likelihood Calibration with Human Feedback [[pdf]](https://arxiv.org/pdf/2305.10425)  
## divergence
f-DPO - Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints [[pdf]](https://arxiv.org/pdf/2309.16240)  
EXO - Towards Efficient and Exact Optimization of Language Model Alignment [[pdf]](https://arxiv.org/pdf/2402.00856)  
## sequence/token level optimization
R-DPO - Disentangling Length from Quality in Direct Preference Optimization [[pdf]](https://arxiv.org/pdf/2403.19159)  
Sim-DPO - SimPO: Simple Preference Optimization with a Reference-Free Reward [[pdf]](https://arxiv.org/pdf/2405.14734)  
PCO - Some things are more CRINGE than others:Iterative Preference Optimization with the Pairwise Cringe Loss [[pdf]](https://arxiv.org/pdf/2312.16682)  
DPO - From r to Q∗: Your Language Model is Secretly a Q-Function [[pdf]](https://arxiv.org/pdf/2404.12358)  
TDPO - Token-level Direct Preference Optimization [[pdf]](https://arxiv.org/pdf/2404.11999)  
## other method for training
ORPO - ORPO: Monolithic Preference Optimization without Reference Model [[pdf]](https://openreview.net/pdf?id=XNzfEFbEJB3)  
MPO - Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model [[pdf]](https://arxiv.org/pdf/2403.19443)  
ICDPO - ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization [[pdf]](https://arxiv.org/pdf/2402.09320)  
ATO - Distributional Preference Alignment of LLMs via Optimal Transport [[pdf]](https://arxiv.org/pdf/2406.05882)  
VPO - Value-Incentivized Preference Optimization:A Unified Approach to Online and Offline RLHF [[pdf]](https://arxiv.org/pdf/2405.19320)  
## likelihood decrease/overfitting
NCA - Noise Contrastive Alignment of Language Models with Explicit Rewards [[pdf]](https://arxiv.org/pdf/2402.05369)  
DPOP - Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive [[pdf]](https://arxiv.org/pdf/2402.13228)  
IPO - A General Theoretical Paradigm to Understand Learning from Human Preferences [[pdf]](https://proceedings.mlr.press/v238/gheshlaghi-azar24a/gheshlaghi-azar24a.pdf)  
d-DPO - Robust Preference Optimization through Reward Model Distillation [[pdf]](https://arxiv.org/pdf/2405.19316)  
## denoise
r-DPO - Provably Robust DPO: Aligning Language Models with Noisy Feedback [[pdf]](https://arxiv.org/pdf/2403.00409)  
Impact of Preference Noise on the Alignment Performance of Generative Language Models [[pdf]](https://arxiv.org/pdf/2404.09824)  
## self-play DPO
Nash Learning from Human Feedback [[pdf]](https://arxiv.org/pdf/2312.00886?trk=organization_guest_main-feed-card-text)  
Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences [[pdf]](https://arxiv.org/pdf/2404.03715)  
Human Alignment of Large Language Models through Online Preference Optimisation [[pdf]](https://arxiv.org/pdf/2403.08635)  
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models [[pdf]](https://arxiv.org/pdf/2401.01335)  
Self-Play Preference Optimization for Language Model Alignment [[pdf]](https://arxiv.org/pdf/2405.00675)  
A Minimaximalist Approach to Reinforcement Learning from Human Feedback [[pdf]](https://arxiv.org/pdf/2401.04056)  
## multi objective
Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment [[pdf]](https://arxiv.org/pdf/2402.19085)  
Direct Preference Optimization With Unobserved Preference Heterogeneity [[pdf]](https://arxiv.org/pdf/2405.15065)  
Group Robust Preference Optimization in Reward-free RLHF [[pdf]](https://arxiv.org/pdf/2405.20304)  
Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives [[pdf]](https://arxiv.org/pdf/2405.17956)  
Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions [[pdf]](https://arxiv.org/pdf/2405.14953)  
SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling [[pdf]](https://arxiv.org/pdf/2405.12739)  
## analysis paper
A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity [[pdf]](https://arxiv.org/pdf/2401.01967)  
Generalized Preference Optimization: A Unified Approach to Offline Alignment [[pdf]](https://arxiv.org/pdf/2402.05749)  
Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks [[pdf]](https://arxiv.org/pdf/2404.14723)  
Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study [[pdf]](https://arxiv.org/pdf/2404.10719)  
Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint [[pdf]](https://openreview.net/pdf?id=c1AKcA6ry1)  
Policy Optimization in RLHF: The Impact of Out-of-preference Data [[pdf]](https://arxiv.org/pdf/2312.10584)  
Preference Learning Algorithms Do Not Learn Preference Rankings [[pdf]](https://arxiv.org/pdf/2405.19534)  
Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences [[pdf]](https://arxiv.org/pdf/2403.01857)  
Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective [[pdf]](https://arxiv.org/pdf/2404.04626)  
Understanding the performance gap between online and offline alignment algorithms [[pdf]](https://arxiv.org/pdf/2405.08448)  
When is RL better than DPO in RLHF? A Representation and Optimization Perspective [[pdf]](https://openreview.net/pdf?id=lNEFatlsQb)  


